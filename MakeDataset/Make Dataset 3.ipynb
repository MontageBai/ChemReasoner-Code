{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "205e78b4-b033-41a7-beef-1b7d487dfab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Read a JSON file from the specified path and return the parsed object.\n",
    "    \n",
    "    :param file_path: Path to the JSON file\n",
    "    :return: Parsed JSON data (usually a dict or list)\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedf73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import Literal, Tuple, Optional, Dict\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e210c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "YesNo = Literal[\"yes\", \"no\"]\n",
    "\n",
    "\n",
    "JUDGE_SYSTEM_PROMPT = \"\"\"You are a validation model designed for dataset quality control.\n",
    "\n",
    "Your task is to rigorously evaluate the reliability of a large language model (LLM) response by comparing the provided original information (raw) with the model-generated answer (answer).\n",
    "\n",
    "Follow these principles during evaluation:\n",
    "1. You will be given both the original information (raw) and the model output (answer).\n",
    "2. If the original information contains explicit facts, standard answers, or verifiable content, assess whether the answer is consistent with them.\n",
    "3. If the original information is extracted from literature or long-form text without a unique standard answer, focus on evaluating the logical soundness of the reasoning process in the answer.\n",
    "4. Identify any clear logical errors, hallucinations, unsupported claims, contradictions, or misinterpretations of the original information.\n",
    "5. If the answer is logically coherent and reasonably supported by the original information, it may be considered reliable even if the wording is not identical.\n",
    "\n",
    "You must output only a single JSON object and no additional text. The output format must be exactly:\n",
    "{\n",
    "  \"verdict\": \"yes\" or \"no\",\n",
    "  \"reason\": \"a brief explanation of the key reason for your judgment\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "JUDGE_USER_TEMPLATE = \"\"\"Please evaluate the following content.\n",
    "\n",
    "【Original Information (raw)】\n",
    "{raw}\n",
    "\n",
    "【Model Output (answer)】\n",
    "{answer}\n",
    "\n",
    "Determine whether the model output is reliable based on the original information.\n",
    "\n",
    "Return strictly a JSON object in the following format and do not include any additional text:\n",
    "{\n",
    "  \"verdict\": \"yes\" or \"no\",\n",
    "  \"reason\": \"one concise sentence explaining the reason for your decision\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _extract_json_from_text(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Compatible with cases where the judge model occasionally adds explanations before or after the JSON:\n",
    "    Attempt to extract the content between the first `{` and the last `}` and parse it as JSON.\n",
    "    \"\"\"\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        raise ValueError(\"Empty judge response\")\n",
    "\n",
    "    # 直接尝试\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # 截取大括号\n",
    "    l = text.find(\"{\")\n",
    "    r = text.rfind(\"}\")\n",
    "    if l != -1 and r != -1 and r > l:\n",
    "        candidate = text[l:r+1]\n",
    "        return json.loads(candidate)\n",
    "\n",
    "    raise ValueError(f\"Judge response is not valid JSON: {text[:200]}...\")\n",
    "\n",
    "\n",
    "def judge_answer_yesno(\n",
    "    client_check,\n",
    "    raw_info: str,\n",
    "    answer_content: str,\n",
    "    *,\n",
    "    judge_model: str,\n",
    "    temperature: float = 0.9,\n",
    "    max_retries: int = 3,\n",
    "    retry_sleep_sec: float = 1.5,\n",
    ") -> Tuple[YesNo, str, dict]:\n",
    "    \"\"\"\n",
    "    Invoke the new large model to make a judgment and return:\n",
    "    (verdict: 'yes'/'no', reason: str, full_json: dict)\n",
    "\n",
    "    - Built-in simple retry\n",
    "    \"\"\"\n",
    "    user_prompt = JUDGE_USER_TEMPLATE.format(raw=raw_info, answer=answer_content)\n",
    "\n",
    "    last_err: Optional[Exception] = None\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            resp = client_check.chat.completions.create(\n",
    "                model=judge_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": JUDGE_SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "            )\n",
    "\n",
    "            text = resp.choices[0].message.content\n",
    "            obj = _extract_json_from_text(text)\n",
    "\n",
    "            verdict = str(obj.get(\"verdict\", \"\")).strip().lower()\n",
    "            reason = str(obj.get(\"reason\", \"\")).strip()\n",
    "\n",
    "            if verdict not in (\"yes\", \"no\"):\n",
    "                raise ValueError(f\"Invalid verdict: {verdict}\")\n",
    "\n",
    "            return verdict, reason, obj\n",
    "\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(retry_sleep_sec * attempt)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    # On failure: conservatively return 'no' and include the error message to avoid interrupting the pipeline\n",
    "    return \"no\", f\"judge_failed: {last_err}\", {\"verdict\": \"no\", \"reason\": f\"judge_failed: {last_err}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cba390f-6453-4d3a-b57b-39a9785361f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_file(file_path, data):\n",
    "    \"\"\"\n",
    "    Write the given data to a JSON file at the specified path.\n",
    "    \n",
    "    :param file_path: Path to the JSON file\n",
    "    :param data: Data to be written (usually a dict or list)\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b5bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_check_json(checkdir: str, filename: str, verdict: YesNo) -> Dict[str, YesNo]:\n",
    "    os.makedirs(os.path.dirname(checkdir), exist_ok=True)\n",
    "\n",
    "    if os.path.exists(checkdir):\n",
    "        try:\n",
    "            with open(checkdir, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "        except Exception:\n",
    "            data = {}\n",
    "    else:\n",
    "        data = {}\n",
    "\n",
    "    data[filename] = verdict\n",
    "\n",
    "    with open(checkdir, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac909dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_check_detail_json(\n",
    "    checkdir_detail: str,\n",
    "    filename: str,\n",
    "    verdict: YesNo,\n",
    "    reason: str,\n",
    "    judge_obj: dict,\n",
    "    *,\n",
    "    source_path: str = \"\",\n",
    "    txt_path: str = \"\",\n",
    "    raw_info: str = \"\",\n",
    "    answer_content: str = \"\",\n",
    "    preview_len: int = 30000,\n",
    "):\n",
    "    if os.path.exists(checkdir_detail):\n",
    "        try:\n",
    "            with open(checkdir_detail, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "        except Exception:\n",
    "            data = {}\n",
    "    else:\n",
    "        data = {}\n",
    "\n",
    "    data[filename] = {\n",
    "        \"verdict\": verdict,\n",
    "        \"reason\": reason,\n",
    "        \"judge\": judge_obj,\n",
    "        \"source_path\": source_path,\n",
    "        \"txt_path\": txt_path,\n",
    "        \"ts\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"raw_preview\": (raw_info[:preview_len] if raw_info else \"\"),\n",
    "        \"answer_preview\": (answer_content[:preview_len] if answer_content else \"\"),\n",
    "    }\n",
    "\n",
    "    os.makedirs(os.path.dirname(checkdir_detail), exist_ok=True)\n",
    "    with open(checkdir_detail, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6185386-7945-4ae4-91f8-9a5ee44548a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepseek_qa(question, answer):\n",
    "    reasoning_content = \"\"  # define the complete reasoning process\n",
    "    answer_content = \"\"     # define the complete response\n",
    "    is_answering = False    # determine whether the reasoning has ended and the response has started\n",
    "\n",
    "    # create chat completion request\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"deepseek-r1\",  # using deepseek-r1 here as an example; replace with another model if needed\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        stream=True\n",
    "        # Uncomment the following to include token usage in the final chunk\n",
    "        # stream_options={\n",
    "        #     \"include_usage\": True\n",
    "        # }\n",
    "    )\n",
    "\n",
    "    # print(\"\\n\" + \"=\" * 20 + \"Reasoning Process\" + \"=\" * 20 + \"\\n\")\n",
    "\n",
    "    for chunk in stream:\n",
    "        # handle usage info\n",
    "        if not getattr(chunk, 'choices', None):\n",
    "            # print(\"\\n\" + \"=\" * 20 + \"Token Usage\" + \"=\" * 20 + \"\\n\")\n",
    "            print(chunk.usage)\n",
    "            continue\n",
    "\n",
    "        delta = chunk.choices[0].delta\n",
    "\n",
    "        # check if reasoning_content attribute exists\n",
    "        if not hasattr(delta, 'reasoning_content'):\n",
    "            continue\n",
    "\n",
    "        # handle empty content case\n",
    "        if not getattr(delta, 'reasoning_content', None) and not getattr(delta, 'content', None):\n",
    "            continue\n",
    "\n",
    "        # handle the start of the answer\n",
    "        if not getattr(delta, 'reasoning_content', None) and not is_answering:\n",
    "            print(\"\\n\" + \"=\" * 20 + \"Complete Response\" + \"=\" * 20 + \"\\n\")\n",
    "            is_answering = True\n",
    "\n",
    "        # handle reasoning process\n",
    "        if getattr(delta, 'reasoning_content', None):\n",
    "            print(delta.reasoning_content, end='', flush=True)\n",
    "            reasoning_content += delta.reasoning_content\n",
    "        # handle response content\n",
    "        elif getattr(delta, 'content', None):\n",
    "            print(delta.content, end='', flush=True)\n",
    "            answer_content += delta.content\n",
    "\n",
    "    # If you need to print the complete content, leave the following uncommented\n",
    "\n",
    "    print(\"=\" * 20 + \"Complete Reasoning Process\" + \"=\" * 20 + \"\\n\")\n",
    "    print(reasoning_content)\n",
    "    print(\"=\" * 20 + \"Complete Response\" + \"=\" * 20 + \"\\n\")\n",
    "    print(answer_content)\n",
    "\n",
    "    verdict, reason, judge_obj = judge_answer_yesno(\n",
    "        client_check,\n",
    "        raw_info=answer,\n",
    "        answer_content=answer_content,\n",
    "        judge_model=\"ModelName\",   # Replace with your reviewer model name\n",
    "\n",
    "    )\n",
    "\n",
    "    # Summary: filename -> yes/no\n",
    "    update_check_json(checkdir, os.path.basename(file_path), verdict)\n",
    "\n",
    "    # Details: filename -> verdict / reason / original judge JSON\n",
    "    update_check_detail_json(\n",
    "        checkdir_detail,\n",
    "        os.path.basename(file_path),\n",
    "        verdict,\n",
    "        reason,\n",
    "        judge_obj,\n",
    "        source_path=file_path,              \n",
    "        txt_path=file_path,                 \n",
    "        raw_info=answer,\n",
    "        answer_content=answer_content,\n",
    "    )\n",
    "\n",
    "    return reasoning_content, answer_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a9fee2-4f46-4c09-8599-9569de946ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_json_file(jsonfile, jsondir):\n",
    "    filepath = f\"{jsondir}\\\\{jsonfile}\"\n",
    "    json_data = read_json_file(filepath)\n",
    "    print(f\"{jsonfile} is running\")\n",
    "    reasoning_content, answer = deepseek_qa(json_data['message_1'], json_data['message_1'])\n",
    "    \n",
    "    json_data[\"reasoning_content\"] = reasoning_content\n",
    "    json_data[\"content\"] = answer\n",
    "    \n",
    "    write_json_file(filepath, json_data)\n",
    "    print(f\"{jsonfile} is done\")\n",
    "    \n",
    "def main(jsondir):\n",
    "    with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "        for jsonfile in os.listdir(jsondir):\n",
    "            \n",
    "            executor.submit(process_json_file, jsonfile, jsondir)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83085229-6df0-445c-8ad4-b4dea4b39d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "024_008_014.json is running\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "client = OpenAI(\n",
    "    # If the environment variable is not configured, replace the following line with your Bailian API Key, e.g., api_key=\"sk-xxx\",\n",
    "    api_key=\"\",  # How to get an API Key: https://help.aliyun.com/zh/model-studio/developer-reference/get-api-key\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "\n",
    "jsondir = r\"\"\n",
    "txtdir = r\"\"\n",
    "checkdir = r\"\"\n",
    "checkdir_detail = r\"\"\n",
    "main(jsondir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c611edd-4948-448a-9ff1-3a74870abe8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
