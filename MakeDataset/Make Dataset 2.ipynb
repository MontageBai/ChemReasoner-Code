{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c40caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import Literal, Tuple, Optional, Dict\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129ecbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "YesNo = Literal[\"yes\", \"no\"]\n",
    "\n",
    "\n",
    "JUDGE_SYSTEM_PROMPT = \"\"\"You are a validation model designed for dataset quality control.\n",
    "\n",
    "Your task is to rigorously evaluate the reliability of a large language model (LLM) response by comparing the provided original information (raw) with the model-generated answer (answer).\n",
    "\n",
    "Follow these principles during evaluation:\n",
    "1. You will be given both the original information (raw) and the model output (answer).\n",
    "2. If the original information contains explicit facts, standard answers, or verifiable content, assess whether the answer is consistent with them.\n",
    "3. If the original information is extracted from literature or long-form text without a unique standard answer, focus on evaluating the logical soundness of the reasoning process in the answer.\n",
    "4. Identify any clear logical errors, hallucinations, unsupported claims, contradictions, or misinterpretations of the original information.\n",
    "5. If the answer is logically coherent and reasonably supported by the original information, it may be considered reliable even if the wording is not identical.\n",
    "\n",
    "You must output only a single JSON object and no additional text. The output format must be exactly:\n",
    "{\n",
    "  \"verdict\": \"yes\" or \"no\",\n",
    "  \"reason\": \"a brief explanation of the key reason for your judgment\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "JUDGE_USER_TEMPLATE = \"\"\"Please evaluate the following content.\n",
    "\n",
    "【Original Information (raw)】\n",
    "{raw}\n",
    "\n",
    "【Model Output (answer)】\n",
    "{answer}\n",
    "\n",
    "Determine whether the model output is reliable based on the original information.\n",
    "\n",
    "Return strictly a JSON object in the following format and do not include any additional text:\n",
    "{\n",
    "  \"verdict\": \"yes\" or \"no\",\n",
    "  \"reason\": \"one concise sentence explaining the reason for your decision\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _extract_json_from_text(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Compatible with cases where the judge model occasionally adds explanations before or after the JSON:\n",
    "    Attempt to extract the content between the first `{` and the last `}` and parse it as JSON.\n",
    "    \"\"\"\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        raise ValueError(\"Empty judge response\")\n",
    "\n",
    "    # 直接尝试\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # 截取大括号\n",
    "    l = text.find(\"{\")\n",
    "    r = text.rfind(\"}\")\n",
    "    if l != -1 and r != -1 and r > l:\n",
    "        candidate = text[l:r+1]\n",
    "        return json.loads(candidate)\n",
    "\n",
    "    raise ValueError(f\"Judge response is not valid JSON: {text[:200]}...\")\n",
    "\n",
    "\n",
    "def judge_answer_yesno(\n",
    "    client_check,\n",
    "    raw_info: str,\n",
    "    answer_content: str,\n",
    "    *,\n",
    "    judge_model: str,\n",
    "    temperature: float = 0.9,\n",
    "    max_retries: int = 3,\n",
    "    retry_sleep_sec: float = 1.5,\n",
    ") -> Tuple[YesNo, str, dict]:\n",
    "    \"\"\"\n",
    "    Invoke the new large model to make a judgment and return:\n",
    "    (verdict: 'yes'/'no', reason: str, full_json: dict)\n",
    "\n",
    "    - Built-in simple retry\n",
    "    \"\"\"\n",
    "    user_prompt = JUDGE_USER_TEMPLATE.format(raw=raw_info, answer=answer_content)\n",
    "\n",
    "    last_err: Optional[Exception] = None\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            resp = client_check.chat.completions.create(\n",
    "                model=judge_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": JUDGE_SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "            )\n",
    "\n",
    "            text = resp.choices[0].message.content\n",
    "            obj = _extract_json_from_text(text)\n",
    "\n",
    "            verdict = str(obj.get(\"verdict\", \"\")).strip().lower()\n",
    "            reason = str(obj.get(\"reason\", \"\")).strip()\n",
    "\n",
    "            if verdict not in (\"yes\", \"no\"):\n",
    "                raise ValueError(f\"Invalid verdict: {verdict}\")\n",
    "\n",
    "            return verdict, reason, obj\n",
    "\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(retry_sleep_sec * attempt)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    # On failure: conservatively return 'no' and include the error message to avoid interrupting the pipeline\n",
    "    return \"no\", f\"judge_failed: {last_err}\", {\"verdict\": \"no\", \"reason\": f\"judge_failed: {last_err}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac3510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_file(file_path):\n",
    "    \"\"\"\n",
    "    Read the specified text file and return its contents.\n",
    "    \n",
    "    :param file_path: Path to the text file\n",
    "    :return: The content of the file as a string\n",
    "    \"\"\"\n",
    "    # Use the with statement to open the file, ensuring it is properly closed afterwards\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Use the read() method to read all contents of the file into a string variable\n",
    "        data = file.read()\n",
    "    # print(type(data))\n",
    "    # At this point, the 'data' variable contains all the contents of the file.\n",
    "    # Note: If the file has multiple lines, 'data' will also include newline characters '\\n'.\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa9474f5-8c1a-4224-82e5-7cd844f3b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_txt_file(file_path, data):\n",
    "    \"\"\"\n",
    "    Write the given data to a text file at the specified path.\n",
    "    \n",
    "    :param file_path: Path to the text file\n",
    "    :param data: Data to be written (string)\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f264a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_check_json(checkdir: str, filename: str, verdict: YesNo) -> Dict[str, YesNo]:\n",
    "    os.makedirs(os.path.dirname(checkdir), exist_ok=True)\n",
    "\n",
    "    if os.path.exists(checkdir):\n",
    "        try:\n",
    "            with open(checkdir, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "        except Exception:\n",
    "            data = {}\n",
    "    else:\n",
    "        data = {}\n",
    "\n",
    "    data[filename] = verdict\n",
    "\n",
    "    with open(checkdir, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3085ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_check_detail_json(\n",
    "    checkdir_detail: str,\n",
    "    filename: str,\n",
    "    verdict: YesNo,\n",
    "    reason: str,\n",
    "    judge_obj: dict,\n",
    "    *,\n",
    "    source_path: str = \"\",\n",
    "    txt_path: str = \"\",\n",
    "    raw_info: str = \"\",\n",
    "    answer_content: str = \"\",\n",
    "    preview_len: int = 30000,\n",
    "):\n",
    "    if os.path.exists(checkdir_detail):\n",
    "        try:\n",
    "            with open(checkdir_detail, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "        except Exception:\n",
    "            data = {}\n",
    "    else:\n",
    "        data = {}\n",
    "\n",
    "    data[filename] = {\n",
    "        \"verdict\": verdict,\n",
    "        \"reason\": reason,\n",
    "        \"judge\": judge_obj,\n",
    "        \"source_path\": source_path,\n",
    "        \"txt_path\": txt_path,\n",
    "        \"ts\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"raw_preview\": (raw_info[:preview_len] if raw_info else \"\"),\n",
    "        \"answer_preview\": (answer_content[:preview_len] if answer_content else \"\"),\n",
    "    }\n",
    "\n",
    "    os.makedirs(os.path.dirname(checkdir_detail), exist_ok=True)\n",
    "    with open(checkdir_detail, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b0451de-be86-49db-9d51-17e727348340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_qa_pairs(text):\n",
    "    \"\"\"\n",
    "    Extract question and answer pairs from the given text, supporting two different formats,\n",
    "    and remove specific begin and end markers.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text containing questions and answers.\n",
    "    \n",
    "    Returns:\n",
    "        list: Each element is a sublist containing a question and its corresponding answer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove extra spaces or line breaks between the start and end markers\n",
    "        clean_text = text.split('<|begin_of_questions_and_answers|>\\n')[1].split('<|end_of_questions_and_answers|>')[0]\n",
    "    except IndexError:\n",
    "        raise ValueError(\"The text does not contain the expected begin and end markers.\")\n",
    "    \n",
    "    # Define regex patterns for two different formats\n",
    "    pattern_no_markdown = r'\\d+\\.\\sQuestion:\\s(.*?)\\s+Answer:\\s(.*?)(?=\\n\\d+\\.|\\Z)'\n",
    "    pattern_markdown = r'\\d+\\.\\s\\*\\*Question:\\*\\*\\s(.*?)\\s+\\*\\*Answer:\\*\\*\\s(.*?)(?=\\n\\d+\\.|\\Z)'\n",
    "\n",
    "    # Find question-answer pairs without Markdown markers\n",
    "    qa_pairs_no_md = re.findall(pattern_no_markdown, clean_text, re.DOTALL | re.MULTILINE)\n",
    "    # Find question-answer pairs with Markdown markers and remove the Markdown symbols\n",
    "    qa_pairs_md = re.findall(pattern_markdown, clean_text, re.DOTALL | re.MULTILINE)\n",
    "    qa_pairs_md_cleaned = [[q.replace(\"**\", \"\").strip(), a.replace(\"**\", \"\").strip()] for q, a in qa_pairs_md]\n",
    "\n",
    "    # Combine results\n",
    "    qa_list = [[q.strip(), a.strip()] for q, a in qa_pairs_no_md] + qa_pairs_md_cleaned\n",
    "    \n",
    "    return qa_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d68982-9285-4972-a887-92231f267207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepseek_qa(txtfile, txtdir):\n",
    "    file_path = f\"{txtdir}\\\\{txtfile}\"\n",
    "    txt = read_txt_file(file_path)\n",
    "    txt2 = read_txt_file(f\"{qadir}\\\\{txtfile}\")\n",
    "    qa_list = extract_qa_pairs(txt2)\n",
    "    \n",
    "    count = 0\n",
    "    print(f\"{txtfile} is running\")\n",
    "\n",
    "    for qa in qa_list:\n",
    "        count += 1\n",
    "        reasoning_content = \"\"  # define the complete reasoning process\n",
    "        answer_content = \"\"     # define the complete response\n",
    "        is_answering = False    # determine whether the reasoning has ended and the response has started\n",
    "        # print(qa[0])\n",
    "        # create chat completion request\n",
    "        stream = client.chat.completions.create(\n",
    "            model=\"qwen-turbo-latest\",  # using qwen-turbo-latest here as an example; replace with another model if needed\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"Based on a comprehensive review in the field of Metal-Organic Frameworks (MOFs) and related questions, generate a detailed and complete chain of scientific reasoning. Ensure that your reasoning process is rigorous and logically coherent, utilizing scientific theories and facts for analysis. The chain of reasoning can be open and flexible, not confined to a rigid structure, but it should clearly indicate the beginning and end of the reasoning.\n",
    "\n",
    "Please use `<|begin_of_thought|>` to mark the start of the reasoning chain and `<|end_of_thought|>` to mark the end.\n",
    "\n",
    "Don't mention \"this literature show\" or \"this review show\" or anything like that. This is very important. Even if you use the literature, your answer should still give the other person a style of thinking that is all about you.\n",
    "\n",
    "The thought chain is as detailed as possible.\n",
    "---\n",
    "\n",
    "**Example Structure:**\n",
    "\n",
    "1. **Understanding the Background:** Briefly explain the background information and main questions.\n",
    "  \n",
    "2. **Application of Knowledge:** Invoke relevant scientific principles and known facts related to the problem.\n",
    "  \n",
    "3. **Analysis Integration:** Integrate key information from the review into the analysis process.\n",
    "  \n",
    "4. **Reasoning Expansion:** Use logical reasoning to explore potential paths to a solution.\n",
    "  \n",
    "5. **Solution Evaluation:** Assess the plausibility and feasibility of different solutions.\n",
    "  \n",
    "6. **Conclusion Formation:** Draw clear scientific conclusions or hypotheses.\n",
    "  \n",
    "7. **Open Exploration:** Suggest possible future research directions or applications.\n",
    "  \n",
    "\n",
    "**Open Thought Chain Template:**\n",
    "\n",
    "<|begin_of_thought|>\n",
    "\n",
    "1. Preliminary Analysis: Clarify the subject and background information.\n",
    "  \n",
    "2. Theoretical Application: Identify and apply relevant scientific theories to support the analysis.\n",
    "  \n",
    "3. Logical Step-by-Step Reasoning: Gradually expand the reasoning, using review information to deepen the analysis.\n",
    "  \n",
    "4. Possibility Discussion: Explore potential conclusions and hypotheses, considering various scientific perspectives.\n",
    "  \n",
    "5. Result Summary: Summarize analysis results to form clear scientific conclusions.\n",
    "  \n",
    "6. Exploration Directions: Propose possible future research directions or application areas.\n",
    "  \n",
    "\n",
    "<|end_of_thought|>\n",
    "\n",
    "Question:\n",
    "{qa[0]}\n",
    "\n",
    "Answer:\n",
    "{qa[1]}\n",
    "\n",
    "There is Artical:\n",
    "{txt}\"\"\"}\n",
    "        ],\n",
    "        stream=True\n",
    "        # Uncomment the following to include token usage in the final chunk\n",
    "        # stream_options={\n",
    "        #     \"include_usage\": True\n",
    "        # }\n",
    "    )\n",
    "\n",
    "\n",
    "        for chunk in stream:\n",
    "            # Handle usage information\n",
    "            if not getattr(chunk, 'choices', None):\n",
    "                print(\"\\n\" + \"=\" * 20 + \"Token Usage\" + \"=\" * 20 + \"\\n\")\n",
    "                print(chunk.usage)\n",
    "                continue\n",
    "    \n",
    "            delta = chunk.choices[0].delta\n",
    "    \n",
    "            # Handle response content\n",
    "            if getattr(delta, 'content', None):\n",
    "                # print(delta.content, end='', flush=True)\n",
    "                answer_content += delta.content\n",
    "    \n",
    "        # If you need to print the full content, uncomment the following\n",
    "    \n",
    "        # print(\"=\" * 20 + \"Complete Response\" + \"=\" * 20 + \"\\n\")\n",
    "        # print(answer_content)\n",
    "        content = f\"\"\"<|begin_of_question|>\\n\\n{qa[0]}\\n\\n<|end_of_question|>\\n\\n<|begin_of_answer|>\\n\\n{qa[1]}\\n\\n<|end_of_answer|>\\n\\n{answer_content}\"\"\"\n",
    "        print(content)\n",
    "        print(\"+++++++++++++++++++\")\n",
    "        print(txt)\n",
    "        verdict, reason, judge_obj = judge_answer_yesno(\n",
    "            client_check,\n",
    "            raw_info=txt,\n",
    "            answer_content=content,\n",
    "            judge_model=\"ModelName\",   # Replace with your reviewer model name\n",
    "\n",
    "        )\n",
    "\n",
    "        # Summary: filename -> yes/no\n",
    "        update_check_json(checkdir, os.path.basename(file_path), verdict)\n",
    "\n",
    "        # Details: filename -> verdict / reason / original judge JSON\n",
    "        update_check_detail_json(\n",
    "            checkdir_detail,\n",
    "            os.path.basename(file_path),\n",
    "            verdict,\n",
    "            reason,\n",
    "            judge_obj,\n",
    "            source_path=file_path,              \n",
    "            txt_path=file_path,                 \n",
    "            raw_info=txt,\n",
    "            answer_content=content,\n",
    "        )\n",
    "        write_txt_file(f\"{file_path[:-4]}_{count}.txt\", content)\n",
    "        # print(f\"file_path[:-4]_{count}.txt\")\n",
    "    return f\"{txtfile} is done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7a20cf6-7a51-45c3-aaf6-5f136f8272b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(txtdir):\n",
    "    futures = []\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        for txtfile in os.listdir(txtdir):\n",
    "            if txtfile.endswith('.txt'):  # ensure only text files are processed\n",
    "                future = executor.submit(deepseek_qa, txtfile, txtdir)\n",
    "                futures.append(future)\n",
    "        \n",
    "        # collect results\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                print(result)\n",
    "            except Exception as e:\n",
    "                print(f\"An exception occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb798f6d-2ca1-4555-a331-3cb9e61e441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "client = OpenAI(\n",
    "    # If the environment variable is not configured, replace the following line with your Bailian API Key, e.g., api_key=\"sk-xxx\",\n",
    "    api_key=\"\",  # How to get an API Key: https://help.aliyun.com/zh/model-studio/developer-reference/get-api-key\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "client_check = OpenAI(\n",
    "    # If the environment variable is not configured, replace the following line with your Bailian API Key, e.g., api_key=\"sk-xxx\",\n",
    "    api_key=\"\",  # How to get an API Key: https://help.aliyun.com/zh/model-studio/developer-reference/get-api-key\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "qadir = \"F:\\\\Working\\\\ModelDistillation\\\\qa\"\n",
    "txtdir = \"F:\\\\Working\\\\ModelDistillation\\\\review\"\n",
    "checkdir = \"F:\\\\Working\\\\ModelDistillation\\\\\\\\DDrevise\\\\test2\\\\check.json\"\n",
    "checkdir_detail = r\"F:\\Working\\ModelDistillation\\DDrevise\\test2\\\\check_detail.json\"\n",
    "main(txtdir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
